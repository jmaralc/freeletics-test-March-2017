{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We pushed a change within our AI-coach, and we want to study its impact on long term retention as well as short term satisfaction.\n",
    "\n",
    "### Suggest meaningful metric(s) to study this use case. How would you go about designing them? What are the set of requirements here?\n",
    "There should be different metrics for the short term and for the long term satisfaction. \n",
    "For the short term some possible metrics could be:\n",
    "\n",
    "    - Deviation of the mean usage time from the previous before/after the change\n",
    "    - Distribution of usage\n",
    "    - Rate of recommendation of the application within the social networks (freeletic network, NPS, Net Promoter Score)\n",
    "    - Rate of mentions in social networks, like twitter, facebook or instagram and their interactions (like, comments, etc)\n",
    "    - Rate of births and deaths in the AI-coach. The rate of new customer and the rate the customer give up with this application\n",
    "\n",
    "For the long term some possible metrics could be:\n",
    "    - Direct surveys to the customer. Users need some time to provide unbiased surveys that is why surveys are better in the long term. \n",
    "    - Subscription time to the service. The longer a user keep subscribe to the service the better will be their satisfaction.\n",
    "    - Continuity of the subscription. Continuity in the subscription line of a customer will point with better levels of satisfaction.\n",
    "    - Stable or growing usage of the AI-coach. Checking the historic records to check the variance of usage. Longer variances will mean more commitment with the training and coaching. It could be correlated with the subscription rate and continuity.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming we have traffic of X users/Day receiving coach weeks from the coach. What is a good way to test the hypothesis that the implemented measure retains users longer in their 4th calendar week?\n",
    "\n",
    "One way to test this will be to sample the mean number of users within the first week and do the same the 4th week. Then we can compare whether the two means are significant similar with a <b>paired t-test</b>. Because we are talking about the same group (an hence there is a dependency) at different points of time, this seems the best option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming that retention in week 4 turns out to be at Y% for the group who saw the new treatment, and Z% who did not see the treatment: name, setup and design a test to validate the effectiveness of this measure.\n",
    "\n",
    "A test to validate this measurement (the different retention percentage between the two groups) will be a <b>Welch's t-test</b>. It is similar to my suggestion to the case of study 1. This test is a good option when we have two different populations, with different variance and different population size like is the current case.  \n",
    "\n",
    "We will have two hypothesis, the called null hypothesis and the alternative hypothesis.\n",
    "The null hypothesis will be:\n",
    "\n",
    "$H_0:$ the means of the retention the two groups are the same\n",
    "\n",
    "and hence the alternative hypothesis will be:\n",
    "\n",
    "$H_1:$ the means of the retention the two groups are the different\n",
    "\n",
    "Then if the $t^*$ is below (within the distribution of the null hypothesis) than the t por the 5% of significant level ($t_{0.05}$) we could discard the null hypothesis and claim that the two different values are significant and likely due to the fact of having use the new treatment. In the other hand, if $t^*$ is not below that level the null hypothesis could not be rejected and hence we can not make any statement about the different retention percentages. This could be use as a confirmation that the retention is independent of the treatment and hence this has no impact on the customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the set of formulas that you would need to validate the hypothesis.\n",
    "\n",
    "\n",
    "The set of formulas to check how significant are our results wihtin the distribution of the null hypothesis are:\n",
    "\n",
    "$\\Huge t^*=\\frac{E[Y]-E[Z]}{\\sqrt{\\frac{s_{Y}^2}{N_Y}+\\frac{s_{Z}^2}{N_Z}}}$\n",
    "\n",
    "where:\n",
    "\n",
    "$E[Y]$ and $E[Z]$ are the sample mean of each retention percentage \n",
    "\n",
    "$N_y$ and $N_z$ are the sample size of each population\n",
    "\n",
    "$s_y$ and $s_z$ are the standard deviation of each population\n",
    "\n",
    "The degrees of freedom are given by the formula:\n",
    "\n",
    "$\\Huge ddof=\\frac{(\\frac{s_{y}^2}{N_{y}}+\\frac{s_{z}^2}{N_{z}})^2}{\\frac{s_{y}^4}{N_{y}^2(N_{y}-1)}+\\frac{s_{z}^4}{N_{z}^2(N_{z}-1)}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we roll back this feature? Why? Why not? Please provide a set of numbers that you used for this decision. How confident are you in this decision?\n",
    "\n",
    "This question has been addressed before. \n",
    "\n",
    "Then if the $t^*$ is below (within the distribution of the null hypothesis) than the t por the 5% of significant level ($t_{0.05}$) we could discard the null hypothesis and claim that the two different values are significant and likely due to the fact of having use the new treatment. In the other hand, if $t^*$ is not below that level the null hypothesis could not be rejected and hence we can not make any statement about the different retention percentages. This could be use as a confirmation that the retention is independent of the treatment and hence this has no impact on the customer experience.\n",
    "\n",
    "In the last case it will be convenient to roll it back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assuming this feature was a success, how would you go about suggesting a new hypothesis to test? Can you provide a quantitative description to a qualitative dummy values to guide your thought process?\n",
    "\n",
    "Because the new feature was a success (hurra!) increasing the retention of users after 4 weeks the team want to know what triggered the response of the users to join the new feature. Usually this is done base on advertisement. \n",
    "In the last news-letter the marketing department put the addvertisement of this feature without any picture that support the text. This made the users to join the treatment(coaching,feature) with a measured average rate of X users/day. \n",
    "\n",
    "In the next news-letter due to the impact in customer retention the bussiness intelligent suggest to give a bit more punch to the app and advertise it with a nice picture and some bold text. One week after this we have a measured joining rate of Y users/Day with Y>X.\n",
    "\n",
    "It is hence a good marketing idea to make this add more appealing or is an extra effort that the users do not use for their choices?\n",
    "\n",
    "To check this we have to perform also an statistics test. The welch test will be a good candidate for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neccessary imports\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as grds\n",
    "import scipy \n",
    "from scipy import signal as signal\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 140000.0\n",
      "Y 139930.036106\n",
      "*************************\n",
      "NULL HYPOTHESIS REJECTED\n",
      "*************************\n",
      "Continue with the email campaign\n"
     ]
    }
   ],
   "source": [
    "#Code to give a quantitative description on the previous qualitative dummy problem\n",
    "\n",
    "Total_Customers = 14e6\n",
    "Percentage_of_users = 0.01 #Assuming that 1% of the users use the new feature\n",
    "X = Total_Customers * Percentage_of_users \n",
    "Y = Total_Customers * (Percentage_of_users+ np.random.normal(loc=0,scale=Percentage_of_users*0.001,size=1)) #Assuming half of the customers acces the application\n",
    "\n",
    "print 'X', X\n",
    "print 'Y', Y[0]\n",
    "\n",
    "#Lets model the two incomming distribution as a Poisson distributions\n",
    "T = 7*24 #We observed the distributions for one week\n",
    "                       \n",
    "population_X = stats.poisson.rvs(loc=Percentage_of_users, mu=X, size=T)\n",
    "population_Y = stats.poisson.rvs(loc=Percentage_of_users, mu=Y, size=T)\n",
    "population_customers_arrivals = np.concatenate((population_X, population_Y))\n",
    "\n",
    "                      \n",
    "#Implementation of the Welch test in scipy\n",
    "st,pvalue = stats.ttest_ind(a= population_X, b= population_Y, equal_var=False)  \n",
    "\n",
    "\n",
    "if pvalue<0.05:\n",
    "    print '*************************'\n",
    "    print 'NULL HYPOTHESIS REJECTED'\n",
    "    print '*************************'\n",
    "    print 'Continue with the email campaign'\n",
    "else:\n",
    "    print '****************************'\n",
    "    print 'NULL HYPOTHESIS NOT REJECTED'\n",
    "    print '****************************'\n",
    "    print 'Stop mail campaign and look for new marketing formulas'\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Is there a Bayesian way we can test our hypothesis? How does it possibly differ from the frequentist approach, both qualitatively and quantitatively.\n",
    "\n",
    "Bayesian hypothesis testing works kind of similar to any bayesian inference problem. There is no needs of asymetry in the hypothesis (in the frequentist approach we test the null distribution but rejecting it does not imply that the other hypothesis will be true). It can be used with multiple hypothesis. \n",
    "\n",
    "In the previous case where we have two hypothesis $H_{0}$ and $H_{1}$ we will have:\n",
    "\n",
    "$P(H_{0}|data)=\\frac{P(data|H_{0})P(H_{0})}{P(data)}$\n",
    "\n",
    "and\n",
    "\n",
    "$P(H_{1}|data)=1-P(H_{0}|data)$\n",
    "\n",
    "\n",
    "The probability of the data comes from the marginalization on the two division of the probability subspaces:\n",
    "\n",
    "$P(data)=P(data|H_{0})P(H_{0})+P(data|H_{1})P(H_{1})$\n",
    "\n",
    "The probabilites $P(data|H_{n})$ are the likelihood while the $P(H_{n})$ are the prior probabilities, that can be set to 0.5 in case of two hypothesis with np preference for any of them. In cases in which we do not have a strong belief on the priors one can always compute the Bayse factor (or likelihod ratio) that helps to quantify which hypotheis is more supported by a data set. \n",
    "\n",
    "Quantitatively we can update our posterior with any single new data. Qualitatively we need an expression for the likelyhood which is not always possible. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
